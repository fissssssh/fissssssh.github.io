[{"content":"最近有位朋友说他在接口中使用 File(Stream stream, string contentType) 方法报错，报错内容是\n2023-03-30 18:00:36.8882|ERROR|Microsoft.AspNetCore.Server.Kestrel|Connection id \u0026#34;OHMPHOM94BVEL\u0026#34;, Request id \u0026#34;OHMPHOM94BVEL:00000004\u0026#34;: An unhandled exception was thrown by the application. 大致推测是请求接口前或者接口后出的异常，也没有详细信息，于是要来了一份代码，代码（精简版）如下：\n// ExcelDocument.cs public class ExcelDocument { public static Stream GetFileStream() { using var fs = new FileStream(\u0026#34;appsettings.json\u0026#34;, FileMode.Open, FileAccess.Read); var ms = new MemoryStream(); fs.CopyTo(ms); return ms; } } // HomeController.cs [HttpGet(\u0026#34;GetFile\u0026#34;)] public IActionResult GetFile() { var stream = ExcelDocument.GetFileStream(); // 原本是application/vnd.ms-excel， 我这里写测试返回的appsettings.json就换了一下 return File(stream, \u0026#34;application/json\u0026#34;); } 这段代码看着没啥问题，我试着请求了一下，果然有报错，我的报错如下：\nfail: Microsoft.AspNetCore.Server.Kestrel[13] Connection id \u0026#34;0HMPH2LG4PQU0\u0026#34;, Request id \u0026#34;0HMPH2LG4PQU0:00000004\u0026#34;: An unhandled exception was thrown by the application. System.InvalidOperationException: Response Content-Length mismatch: too few bytes written (0 of 151). 注意错误中多了一行 System.InvalidOperationException: Response Content-Length mismatch: too few bytes written (0 of 151).，这很关键，说明之前的日志模板没有输出异常的详细信息\n根据异常信息来看，响应头 Content-Length 设置为了 151 ，但是 body 中没有写入数据（写入字节数为 0），大致能猜到是没读取到流的内容，为什么呢？创建的 MemoryStream 也没有关闭,答案就在于Stream.CopyTo(Steam)这个方法，让我们来看一下官方文档对这个函数的定义\nReads the bytes from the current stream and writes them to another stream. Both streams positions are advanced by the number of bytes copied. 恍然大悟！原来 CopyTo 会将流的 Position 移动，怪不得读取不到数据\n解决这个问题也很简单，就是在 CopyTo 方法之后将流的指针移动到开始位置（流的最前端）\n// ExcelDocument.cs public class ExcelDocument { public static Stream GetFileStream() { using var fs = new FileStream(\u0026#34;appsettings.json\u0026#34;, FileMode.Open, FileAccess.Read); var ms = new MemoryStream(); fs.CopyTo(ms); ms.Seek(0, SeekOrigin.Begin); return ms; } } $ curl http://localhost:5125/Home/GetFile { \u0026#34;Logging\u0026#34;: { \u0026#34;LogLevel\u0026#34;: { \u0026#34;Default\u0026#34;: \u0026#34;Information\u0026#34;, \u0026#34;Microsoft.AspNetCore\u0026#34;: \u0026#34;Warning\u0026#34; } }, \u0026#34;AllowedHosts\u0026#34;: \u0026#34;*\u0026#34; } 总结下来有两个问题：\n对 Stream.CopyTo 这个方法的不了解，未能在编码的时候注意到并避免（主要原因） 日志记录模板没有配置显示异常详细信息而只显示了 Message，让我们没有足够的报错信息去排查错误 ","permalink":"https://blog.fissssssh.com/posts/how-to-return-stream-in-asp_net_core_webapi-correctly/","summary":"最近有位朋友说他在接口中使用 File(Stream stream, string contentType) 方法报错，报错内容是\n2023-03-30 18:00:36.8882|ERROR|Microsoft.AspNetCore.Server.Kestrel|Connection id \u0026#34;OHMPHOM94BVEL\u0026#34;, Request id \u0026#34;OHMPHOM94BVEL:00000004\u0026#34;: An unhandled exception was thrown by the application. 大致推测是请求接口前或者接口后出的异常，也没有详细信息，于是要来了一份代码，代码（精简版）如下：\n// ExcelDocument.cs public class ExcelDocument { public static Stream GetFileStream() { using var fs = new FileStream(\u0026#34;appsettings.json\u0026#34;, FileMode.Open, FileAccess.Read); var ms = new MemoryStream(); fs.CopyTo(ms); return ms; } } // HomeController.cs [HttpGet(\u0026#34;GetFile\u0026#34;)] public IActionResult GetFile() { var stream = ExcelDocument.GetFileStream(); // 原本是application/vnd.ms-excel， 我这里写测试返回的appsettings.json就换了一下 return File(stream, \u0026#34;application/json\u0026#34;); } 这段代码看着没啥问题，我试着请求了一下，果然有报错，我的报错如下：","title":"如何正确在 ASP.NET Core 中返回流"},{"content":"从 K8s 的容器中拷贝文件可以使用kubectl cp命令，但是对于较大的文件（100M 以上）可能就不是很好使了，拷贝过程经常会出现EOF错误\n对于较大的文件，我是这样操作的：\n压缩\n通过压缩可以减小大文件的体积\ntar -czvf largefile.tar.gz largefile 此操作也可以用于打包整个文件夹，以便达到拷贝文件夹的目的\n分片\n对于体积较大的文件，即使压缩后仍然有好几百 M，这时使用kubectl cp也不一定能够拷贝下来，所以需要对其分割\nsplit -b 50M -d largefile.tar.gz largefile.tar.gz 此操作会将largefile.tar.gz按照 50M/每个文件的大小分割为多个文件，-d选项则是使用数字来作为后缀（默认是英文字母）\n例如：largefile.tar.gz有 220M，则分割后会变为\nlargefile.tar.gz00 # 50M largefile.tar.gz01 # 50M largefile.tar.gz02 # 50M largefile.tar.gz03 # 50M largefile.tar.gz04 # 20M 拷贝\n使用kubectl cp命令拷贝分割后的文件\nkubectl cp \u0026lt;pod-name\u0026gt;:largefile.tar.gz00 largefile.tar.gz00 kubectl cp \u0026lt;pod-name\u0026gt;:largefile.tar.gz00 largefile.tar.gz01 kubectl cp \u0026lt;pod-name\u0026gt;:largefile.tar.gz00 largefile.tar.gz02 kubectl cp \u0026lt;pod-name\u0026gt;:largefile.tar.gz00 largefile.tar.gz03 kubectl cp \u0026lt;pod-name\u0026gt;:largefile.tar.gz00 largefile.tar.gz04 对于多个容器的 Pod,可以使用-c选项指定具体的容器\n合并 使用cat命令合并多个文件\ncat largefile.tar.gz* \u0026gt; largefile.tar.gz 可以使用md5sum命令计算合并后的文件 hash 值与容器内的文件 hash 值是否一致来判断文件是否有缺失\n解压\ntar -xzvf largefile.tar.gz 至此，我就将大文件从 Pod 中拷贝出来了\n","permalink":"https://blog.fissssssh.com/posts/copy-large-file-from-k8s-pod/","summary":"从 K8s 的容器中拷贝文件可以使用kubectl cp命令，但是对于较大的文件（100M 以上）可能就不是很好使了，拷贝过程经常会出现EOF错误\n对于较大的文件，我是这样操作的：\n压缩\n通过压缩可以减小大文件的体积\ntar -czvf largefile.tar.gz largefile 此操作也可以用于打包整个文件夹，以便达到拷贝文件夹的目的\n分片\n对于体积较大的文件，即使压缩后仍然有好几百 M，这时使用kubectl cp也不一定能够拷贝下来，所以需要对其分割\nsplit -b 50M -d largefile.tar.gz largefile.tar.gz 此操作会将largefile.tar.gz按照 50M/每个文件的大小分割为多个文件，-d选项则是使用数字来作为后缀（默认是英文字母）\n例如：largefile.tar.gz有 220M，则分割后会变为\nlargefile.tar.gz00 # 50M largefile.tar.gz01 # 50M largefile.tar.gz02 # 50M largefile.tar.gz03 # 50M largefile.tar.gz04 # 20M 拷贝\n使用kubectl cp命令拷贝分割后的文件\nkubectl cp \u0026lt;pod-name\u0026gt;:largefile.tar.gz00 largefile.tar.gz00 kubectl cp \u0026lt;pod-name\u0026gt;:largefile.tar.gz00 largefile.tar.gz01 kubectl cp \u0026lt;pod-name\u0026gt;:largefile.tar.gz00 largefile.tar.gz02 kubectl cp \u0026lt;pod-name\u0026gt;:largefile.tar.gz00 largefile.tar.gz03 kubectl cp \u0026lt;pod-name\u0026gt;:largefile.tar.gz00 largefile.tar.gz04 对于多个容器的 Pod,可以使用-c选项指定具体的容器\n合并 使用cat命令合并多个文件\ncat largefile.","title":"从 K8s 的 Pod 中拷贝大文件"},{"content":"Newtonsoft.Json 是一个非常受欢迎的 .NET JSON 框架\n一般来说大部分用户用到的方法主要就是 JsonConvert.SerializeObject 和 JsonConvert.DeserializeObject 方法\n前者用于将 .NET 对象 序列化为 JSON 字符串，后者则是将 JSON 字符串反序列化为 .NET 对象\n下面我将讲述一些你可能用过或者没用过的一些小技巧\n填充对象 我现在有一个 {\u0026quot;name\u0026quot;:\u0026quot;fissssssh\u0026quot;} JSON 对象，当将它转为Dictionary\u0026lt;string,string\u0026gt;后，可以通过键名name获取属性值\n然而这个 JSON 现在变成 {\u0026quot;Name\u0026quot;:\u0026quot;fissssssh\u0026quot;}, 无法再通过键名 name 获取属性值\nDictionary 是支持替换键名比较器的，但是 JsonConvert.DeserializeObject 只会调用其无参构造函数\n我们可以手动将 Dictionary 创建出来, 然后通过 JsonConvert.PopulateObject 方法将 JSON 字符串序列化并填充至指定对象，代码如下\nvar json = \u0026#34;{\\\u0026#34;Name\\\u0026#34;:\\\u0026#34;fissssssh\\\u0026#34;}\u0026#34;; // 创建一个键名忽略大小写的字典对象 var dict = new Dictionary\u0026lt;string,string\u0026gt;(StringComparer.InvariantCultureIgnoreCase); JsonConvert.PopulateObject(json, dict); // 使用 name 获取 JSON 中的 Name 属性值 var name = dict[\u0026#34;name\u0026#34;]; 从流进行反序列化 有些情况下我们需要反序列化来自网络请求或者文件中 JSON，它们通常是以流的形式持有，反序列化先将其读取为字符串，当 JSON 内容过大的时候会产生一个巨大的字符串对象，如果此操作比较频繁则会对性能产生比较大的影响\nNewtonsoft.JSON 并没有直接提供从流反序列化的方法，但我们可以这样操作：\nusing var stream = ReadStreamFromFileOrNetwork(); using var sr = new StreamReader(stream); var serializer = JsonSerializer.Create(); return serializer.Deserialize\u0026lt;List\u0026lt;Root\u0026gt;\u0026gt;(new JsonTextReader(sr)); 我使用以下代码对直接从流反序列化和先转为字符串再反序列化做了基准测试进行比较\n[MemoryDiagnoser, ShortRunJob] public class DeserializeFromStreamDirectlyVsDeserializeAfterConvertStreamToString { private byte[] data = null!; [GlobalSetup] public void Setup() { data = File.ReadAllBytes(\u0026#34;large-file.json\u0026#34;); } [Benchmark] public List\u0026lt;Root\u0026gt;? DeserializeFromStreamDirectly() { using var ms = new MemoryStream(data); using var sr = new StreamReader(ms); var serializer = JsonSerializer.Create(); return serializer.Deserialize\u0026lt;List\u0026lt;Root\u0026gt;\u0026gt;(new JsonTextReader(sr)); } [Benchmark] public List\u0026lt;Root\u0026gt;? DeserializeAfterConvertStreamToString() { using var ms = new MemoryStream(data); using var sr = new StreamReader(ms); var json = sr.ReadToEnd(); return JsonConvert.DeserializeObject\u0026lt;List\u0026lt;Root\u0026gt;\u0026gt;(json); } } 测试 JSON 文件来自 json-iterator/test-data/large-file.json\n测试结果如下：\nBenchmarkDotNet=v0.13.5, OS=Windows 11 (10.0.22621.1265/22H2/2022Update/SunValley2) AMD Ryzen 9 5900X, 1 CPU, 24 logical and 12 physical cores .NET SDK=7.0.100 [Host] : .NET 7.0.0 (7.0.22.51805), X64 RyuJIT AVX2 ShortRun : .NET 7.0.0 (7.0.22.51805), X64 RyuJIT AVX2 Job=ShortRun IterationCount=3 LaunchCount=1 WarmupCount=3 Method Mean Error StdDev Gen0 Gen1 Gen2 Allocated DeserializeFromStreamDirectly 119.4 ms 9.73 ms 0.53 ms 4400.0000 2000.0000 800.0000 60.35 MB DeserializeAfterConvertStreamToString 143.5 ms 45.52 ms 2.50 ms 8250.0000 6000.0000 1750.0000 160.15 MB 可以看出时间上面二者没有太大区别，但是在内存分配上足足省了 100MB!\n","permalink":"https://blog.fissssssh.com/posts/newtonsoft-json-tips/","summary":"Newtonsoft.Json 是一个非常受欢迎的 .NET JSON 框架\n一般来说大部分用户用到的方法主要就是 JsonConvert.SerializeObject 和 JsonConvert.DeserializeObject 方法\n前者用于将 .NET 对象 序列化为 JSON 字符串，后者则是将 JSON 字符串反序列化为 .NET 对象\n下面我将讲述一些你可能用过或者没用过的一些小技巧\n填充对象 我现在有一个 {\u0026quot;name\u0026quot;:\u0026quot;fissssssh\u0026quot;} JSON 对象，当将它转为Dictionary\u0026lt;string,string\u0026gt;后，可以通过键名name获取属性值\n然而这个 JSON 现在变成 {\u0026quot;Name\u0026quot;:\u0026quot;fissssssh\u0026quot;}, 无法再通过键名 name 获取属性值\nDictionary 是支持替换键名比较器的，但是 JsonConvert.DeserializeObject 只会调用其无参构造函数\n我们可以手动将 Dictionary 创建出来, 然后通过 JsonConvert.PopulateObject 方法将 JSON 字符串序列化并填充至指定对象，代码如下\nvar json = \u0026#34;{\\\u0026#34;Name\\\u0026#34;:\\\u0026#34;fissssssh\\\u0026#34;}\u0026#34;; // 创建一个键名忽略大小写的字典对象 var dict = new Dictionary\u0026lt;string,string\u0026gt;(StringComparer.InvariantCultureIgnoreCase); JsonConvert.PopulateObject(json, dict); // 使用 name 获取 JSON 中的 Name 属性值 var name = dict[\u0026#34;name\u0026#34;]; 从流进行反序列化 有些情况下我们需要反序列化来自网络请求或者文件中 JSON，它们通常是以流的形式持有，反序列化先将其读取为字符串，当 JSON 内容过大的时候会产生一个巨大的字符串对象，如果此操作比较频繁则会对性能产生比较大的影响","title":"Newtonsoft.Json 小技巧"},{"content":"在你使用 git 的过程中，你是否有过以下问题：\n提交信息写错了 提交了不该提交的文件（例如某些调试过程产生的文件） …… 如果这些问题仅存在于本地，那么有很多种解决办法，比如使用 commit --amend， rebase 等指令，大不了就 reset --mixed 再重新 commit\n但是如果你过了好几个甚至好几十个提交才发现这些问题，那么用上述方法就不太好解决了\ngit-filter-repo git-filter-repo 是一个用于重写历史提交的多功能工具，它和 git filter-branch 比较相似，但是其性能远远优于后者\ngit 官方也推荐使用 git-filter-repo 而不是 git filter-branch\n安装 前置 git \u0026gt;= 2.22.0 at a minimum; some features require git \u0026gt;= 2.24.0 or later python3 \u0026gt;= 3.5 包管理器安装 PACKAGE_TOOL install git-filter-repo 手动安装 下载最新的 Release 解压后将 git-filter-repo 文件复制到 git --exec-path 文件夹下\n如果你的 python3 指令被命名为 python ，你还需要将 git-filter-repo 文件第一行中的 python3 改为 python（此问题困扰了大部分 Windows 用户）\n例子 本例子使用以下仓库，历史提交信息如下\ngit log --stat commit c13190a52fddf8db5d561659ae01aa32276f865e (HEAD -\u0026gt; master) Author: fissssssh \u0026lt;fissssssh@email.com\u0026gt; Date: Tue Mar 7 12:42:20 2023 +0800 modify a.txt a.txt | 1 + 1 file changed, 1 insertion(+) commit 258ba82351e483d12f3876764feb2451295c3d9d Author: fissssssh \u0026lt;fissssssh@email.com\u0026gt; Date: Tue Mar 7 12:41:43 2023 +0800 commit a.txt .vscode/launch.json | 26 ++++++++++++++++++++++++++ .vscode/tasks.json | 41 +++++++++++++++++++++++++++++++++++++++++ a.txt | 1 + bigfile | Bin 0 -\u0026gt; 209715201 bytes 4 files changed, 68 insertions(+) 删除错误提交的文件 上述仓库中有两个提交，我在初次提交的时候不小心将 .vscode 和 bigfile 提交了，并且后续还追加了一个提交\n现在使用 git-filter-repo 来删除多余的文件，执行以下指令：\ngit filter-repo --path a.txt --path 选项表示只保留此文件，可以使用多个 --path 选项\n类似的选项还有 --path-glob， --path-regex\n使用 --invert-paths 参数可以将选中的文件变为反选\n输出如下：\nAborting: Refusing to destructively overwrite repo history since this does not look like a fresh clone. (expected at most one entry in the reflog for HEAD) Please operate on a fresh clone instead. If you want to proceed anyway, use --force. 意思就是说你这个仓库不是刚克隆了，请使用新克隆的仓库执行操作，如果你硬要操作请加上 --force 选项。我们这里是使用的测试仓库，所以可以不管这条消息，重新执行以下指令：\ngit filter-repo --path a.txt --force 输出如下：\nParsed 2 commits New history written in 0.01 seconds; now repacking/cleaning... Repacking your repo and cleaning out old unneeded objects HEAD 现在位于 a1e776f modify a.txt 枚举对象中: 6, 完成. 对象计数中: 100% (6/6), 完成. 使用 16 个线程进行压缩 压缩对象中: 100% (2/2), 完成. 写入对象中: 100% (6/6), 完成. 总共 6（差异 0），复用 0（差异 0），包复用 0 Completely finished after 0.05 seconds. 让我们重新看一下提交历史\ngit log --stat commit a1e776f3d90e2023fc59e66815d8392151a7f612 (HEAD -\u0026gt; master) Author: fissssssh \u0026lt;fissssssh@email.com\u0026gt; Date: Tue Mar 7 12:42:20 2023 +0800 modify a.txt a.txt | 1 + 1 file changed, 1 insertion(+) commit 57d35f202e92be0fa34c8e771266c0c4e5a65f1a Author: fissssssh \u0026lt;fissssssh@email.com\u0026gt; Date: Tue Mar 7 12:41:43 2023 +0800 commit a.txt a.txt | 1 + 1 file changed, 1 insertion(+) 细心的你会发现提交的 hash 值也变了，这是必然的\n更改提交文件的文件名称 现在我发现 a.txt 这个名称不够好，我想换成 b.txt，只需执行以下指令：\ngit filter-repo --path-rename a.txt:b.txt 输出如下：\nParsed 2 commits New history written in 0.02 seconds; now repacking/cleaning... Repacking your repo and cleaning out old unneeded objects HEAD 现在位于 435a640 modify a.txt 枚举对象中: 6, 完成. 对象计数中: 100% (6/6), 完成. 使用 16 个线程进行压缩 压缩对象中: 100% (2/2), 完成. 写入对象中: 100% (6/6), 完成. 总共 6（差异 0），复用 2（差异 0），包复用 0 Completely finished after 0.05 seconds. 让我们重新看一下提交历史\ngit log --stat commit 435a640f4b8a42bfe06a5695a7ff4dd061f2d3b5 (HEAD -\u0026gt; master) Author: fissssssh \u0026lt;fissssssh@email.com\u0026gt; Date: Tue Mar 7 12:42:20 2023 +0800 modify a.txt b.txt | 1 + 1 file changed, 1 insertion(+) commit 000c714faced2aede9ae18c294ef4b9698cd52be Author: fissssssh \u0026lt;fissssssh@email.com\u0026gt; Date: Tue Mar 7 12:41:43 2023 +0800 commit a.txt b.txt | 1 + 1 file changed, 1 insertion(+) 更改提交信息 文件名改了，提交信息还没变，让我们再来修改提交信息\n修改提交信息需要一个替换文件，创建 expressions.txt 文件：\ncat \u0026lt;\u0026lt;EOF \u0026gt; expressions.txt a.txt==\u0026gt;b.txt EOF 你可以用任何方式去创建文件\n执行以下指令：\ngit filter-repo --replace-message expressions.txt 输出如下：\nParsed 2 commits New history written in 0.02 seconds; now repacking/cleaning... Repacking your repo and cleaning out old unneeded objects HEAD 现在位于 49d497e modify b.txt 枚举对象中: 6, 完成. 对象计数中: 100% (6/6), 完成. 使用 16 个线程进行压缩 压缩对象中: 100% (2/2), 完成. 写入对象中: 100% (6/6), 完成. 总共 6（差异 0），复用 4（差异 0），包复用 0 Completely finished after 0.05 seconds. 让我们重新看一下提交历史：\ngit log --stat commit 49d497e61b0370a6f6e23ed740eaa045be569361 (HEAD -\u0026gt; master) Author: fissssssh \u0026lt;fissssssh@email.com\u0026gt; Date: Tue Mar 7 12:42:20 2023 +0800 modify b.txt b.txt | 1 + 1 file changed, 1 insertion(+) commit baabe05301b0a38057538d47e3b7b899cd348935 Author: fissssssh \u0026lt;fissssssh@email.com\u0026gt; Date: Tue Mar 7 12:41:43 2023 +0800 commit b.txt b.txt | 1 + 1 file changed, 1 insertion(+) 更新至远端仓库 此步骤谨慎操作！谨慎操作！谨慎操作！\n由于历史提交 hash 已变更，只能强制推送\ngit push -f origin master 最后 git-filter-repo 极大的简化了重写 git 提交历史的操作，这里介绍的只是九牛一毛，更多的操作可以参考官方用户手册\n","permalink":"https://blog.fissssssh.com/posts/rewrite-git-repo-history/","summary":"在你使用 git 的过程中，你是否有过以下问题：\n提交信息写错了 提交了不该提交的文件（例如某些调试过程产生的文件） …… 如果这些问题仅存在于本地，那么有很多种解决办法，比如使用 commit --amend， rebase 等指令，大不了就 reset --mixed 再重新 commit\n但是如果你过了好几个甚至好几十个提交才发现这些问题，那么用上述方法就不太好解决了\ngit-filter-repo git-filter-repo 是一个用于重写历史提交的多功能工具，它和 git filter-branch 比较相似，但是其性能远远优于后者\ngit 官方也推荐使用 git-filter-repo 而不是 git filter-branch\n安装 前置 git \u0026gt;= 2.22.0 at a minimum; some features require git \u0026gt;= 2.24.0 or later python3 \u0026gt;= 3.5 包管理器安装 PACKAGE_TOOL install git-filter-repo 手动安装 下载最新的 Release 解压后将 git-filter-repo 文件复制到 git --exec-path 文件夹下\n如果你的 python3 指令被命名为 python ，你还需要将 git-filter-repo 文件第一行中的 python3 改为 python（此问题困扰了大部分 Windows 用户）","title":"重写你的 Git 仓库历史"},{"content":"什么是 HTTP 查询字符串 例如此 URL：https://example.com:80/query?key1=value2\u0026amp;key2=value2\n对其拆解我们可以得到以下部分：\nhttps：协议 :// example.com：域名 :80：端口 /query：路径 ?key1=value2\u0026amp;key2=value2：参数（也称作查询字符串） 如何生成 HTTP 查询字符串 暴力拼接 略\nSystem.Web.HttpUtility var query = HttpUtility.ParseQueryString(string.Empty); query[\u0026#34;a+b\u0026#34;] = \u0026#34;a%b\u0026#34;; query[\u0026#34;b\u0026#34;] = \u0026#34;2+1\u0026#34;; var queryString = query.ToString(); // a+b=a%25b\u0026amp;b=2%2b1 HttpUtility.ParseQueryString(string.Empty) 会返回一个空的NameValueCollection，你只需要往里面填充参数然后调用ToString()即可生成查询字符串\n您不能使用 new NameValueCollection() 来达到同样的效果，因为 HttpUtility.ParseQueryString(string) 返回的实际是 HttpQSCollection 类型，该类型是 NameValueCollection 的派生类型且不对外公开，所以你也无法通过 new 关键字来创建 HttpQSCollection 类型\n此方法生成的查询字符串只会转义 value 且不包含前导字符 ?\nMicrosoft.AspNetCore.Http.QueryString 此方法仅适用于 SDK 为 Microsoft.NET.Sdk.Web 的项目\nvar queryString = QueryString.Create(new Dictionary\u0026lt;string, string?\u0026gt; { [\u0026#34;a+b\u0026#34;] = \u0026#34;a%b\u0026#34;, [\u0026#34;b\u0026#34;] = \u0026#34;2+1\u0026#34;, }).ToString(); // ?a%2Bb=a%25b\u0026amp;b=2%2B1 此方法生成的查询字符串会转义 key 和 value 且包含前导字符 ?\n除了调用 ToString() 也可以调用 ToUriComponent()，二者效果相同\n","permalink":"https://blog.fissssssh.com/posts/generate-http-query-string/","summary":"什么是 HTTP 查询字符串 例如此 URL：https://example.com:80/query?key1=value2\u0026amp;key2=value2\n对其拆解我们可以得到以下部分：\nhttps：协议 :// example.com：域名 :80：端口 /query：路径 ?key1=value2\u0026amp;key2=value2：参数（也称作查询字符串） 如何生成 HTTP 查询字符串 暴力拼接 略\nSystem.Web.HttpUtility var query = HttpUtility.ParseQueryString(string.Empty); query[\u0026#34;a+b\u0026#34;] = \u0026#34;a%b\u0026#34;; query[\u0026#34;b\u0026#34;] = \u0026#34;2+1\u0026#34;; var queryString = query.ToString(); // a+b=a%25b\u0026amp;b=2%2b1 HttpUtility.ParseQueryString(string.Empty) 会返回一个空的NameValueCollection，你只需要往里面填充参数然后调用ToString()即可生成查询字符串\n您不能使用 new NameValueCollection() 来达到同样的效果，因为 HttpUtility.ParseQueryString(string) 返回的实际是 HttpQSCollection 类型，该类型是 NameValueCollection 的派生类型且不对外公开，所以你也无法通过 new 关键字来创建 HttpQSCollection 类型\n此方法生成的查询字符串只会转义 value 且不包含前导字符 ?\nMicrosoft.AspNetCore.Http.QueryString 此方法仅适用于 SDK 为 Microsoft.NET.Sdk.Web 的项目\nvar queryString = QueryString.Create(new Dictionary\u0026lt;string, string?\u0026gt; { [\u0026#34;a+b\u0026#34;] = \u0026#34;a%b\u0026#34;, [\u0026#34;b\u0026#34;] = \u0026#34;2+1\u0026#34;, }).","title":"生成 HTTP 查询字符串"},{"content":"utterances 是基于 GitHub issues 构建的轻量级评论小部件，通过此组件可以将 Github issues 应用于任何网站\n前置条件 一个公开的 github repo 在该 repo 中安装utterances app 简单集成 打开utterances官网\n在 configuration 节点填写 repo 信息等，填写好后会生成对应的组件代码\n我使用的是 PaperMod1 主题，将生成好的代码放入layouts/partials/comments.html\n\u0026lt;!--layouts/partials/comments.html--\u0026gt; \u0026lt;script src=\u0026#34;https://utteranc.es/client.js\u0026#34; repo=\u0026#34;[ENTER REPO HERE]\u0026#34; issue-term=\u0026#34;pathname\u0026#34; theme=\u0026#34;github-light\u0026#34; label=\u0026#34;Comment\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async \u0026gt;\u0026lt;/script\u0026gt; 然后在config.yaml中开启评论功能即可简单集成\nparams: comments: true 动态主题适配 PaperMod 有明亮和暗黑两种颜色模式，而 utterances 的主题是编码时就写在script标签中的，为了使 comments 组件的主题适配 PaperMod 颜色主题，有以下两个步骤：\n主题动态初始化 \u0026lt;!-- layouts/partials/comments.html --\u0026gt; \u0026lt;script\u0026gt; (function () { let theme = localStorage.getItem(\u0026#34;pref-theme\u0026#34;); // already has prefer theme if (theme) { theme = theme === \u0026#34;dark\u0026#34; ? \u0026#34;github-dark\u0026#34; : \u0026#34;github-light\u0026#34;; } // auto detected else { theme = window.matchMedia(\u0026#34;(prefers-color-scheme: dark)\u0026#34;).matches ? \u0026#34;github-dark\u0026#34; : \u0026#34;github-light\u0026#34;; } const comment = document.createElement(\u0026#34;script\u0026#34;); comment.src = \u0026#34;https://utteranc.es/client.js\u0026#34;; comment.crossorigin = \u0026#34;anonymous\u0026#34;; comment.async = true; comment.setAttribute(\u0026#34;repo\u0026#34;, \u0026#34;fissssssh/fissssssh.github.io\u0026#34;); comment.setAttribute(\u0026#34;issue-term\u0026#34;, \u0026#34;pathname\u0026#34;); comment.setAttribute(\u0026#34;label\u0026#34;, \u0026#34;Comment\u0026#34;); comment.setAttribute(\u0026#34;theme\u0026#34;, theme); document.body.appendChild(comment); })(); \u0026lt;/script\u0026gt; 用户未切换主题时，Local Storage 中还没有 pref-theme 的值，需要识别系统主题来确定评论组件的主题\n主题动态切换 \u0026lt;!-- layouts/partials/extend_footer.html --\u0026gt; {{- if (not site.Params.disableThemeToggle) }} \u0026lt;script\u0026gt; document.getElementById(\u0026#34;theme-toggle\u0026#34;).addEventListener(\u0026#34;click\u0026#34;, () =\u0026gt; { const iframe = document.querySelector(\u0026#34;.utterances-frame\u0026#34;); const message = { type: \u0026#34;set-theme\u0026#34;, theme: localStorage.getItem(\u0026#34;pref-theme\u0026#34;) === \u0026#34;light\u0026#34; // if current theme is light, then set comment widget\u0026#39;s theme to dark, otherwise light. ? \u0026#34;github-dark\u0026#34; : \u0026#34;github-light\u0026#34;, }; iframe?.contentWindow.postMessage(message, \u0026#34;https://utteranc.es\u0026#34;); }); \u0026lt;/script\u0026gt; {{- end }} PaperMod 会将当前颜色主题存在 localStorage-\u0026gt;pref-theme 下\n参考 Adaptive dark theme Dynamic theme changing https://github.com/adityatelange/hugo-PaperMod\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://blog.fissssssh.com/posts/integrate-utterances-in-hugo/","summary":"utterances 是基于 GitHub issues 构建的轻量级评论小部件，通过此组件可以将 Github issues 应用于任何网站\n前置条件 一个公开的 github repo 在该 repo 中安装utterances app 简单集成 打开utterances官网\n在 configuration 节点填写 repo 信息等，填写好后会生成对应的组件代码\n我使用的是 PaperMod1 主题，将生成好的代码放入layouts/partials/comments.html\n\u0026lt;!--layouts/partials/comments.html--\u0026gt; \u0026lt;script src=\u0026#34;https://utteranc.es/client.js\u0026#34; repo=\u0026#34;[ENTER REPO HERE]\u0026#34; issue-term=\u0026#34;pathname\u0026#34; theme=\u0026#34;github-light\u0026#34; label=\u0026#34;Comment\u0026#34; crossorigin=\u0026#34;anonymous\u0026#34; async \u0026gt;\u0026lt;/script\u0026gt; 然后在config.yaml中开启评论功能即可简单集成\nparams: comments: true 动态主题适配 PaperMod 有明亮和暗黑两种颜色模式，而 utterances 的主题是编码时就写在script标签中的，为了使 comments 组件的主题适配 PaperMod 颜色主题，有以下两个步骤：\n主题动态初始化 \u0026lt;!-- layouts/partials/comments.html --\u0026gt; \u0026lt;script\u0026gt; (function () { let theme = localStorage.getItem(\u0026#34;pref-theme\u0026#34;); // already has prefer theme if (theme) { theme = theme === \u0026#34;dark\u0026#34; ?","title":"在 Hugo 中集成 utterances 评论组件"},{"content":"dapr 应用程序是一个多进程程序，对于 Visual Studio 来说可能比较困难\n这里介绍两种方法来使用 Visual Studio 调试 dapr 应用程序\n使用 VS 扩展 附加进程调试 使用 VS 扩展 参考 https://github.com/dapr/dotnet-sdk/issues/401#issuecomment-747563695\n准备条件 安装 PowerShell 7 / Core 也可使用 dotnet tool install --global PowerShell 进行安装\n安装 VS 扩展 Microsoft Child Process Debugging Power Tool 2022 调试准备 编辑launchSettings.json\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://json.schemastore.org/launchsettings.json\u0026#34;, \u0026#34;profiles\u0026#34;: { \u0026#34;WebApplication1\u0026#34;: { \u0026#34;commandName\u0026#34;: \u0026#34;Project\u0026#34;, \u0026#34;dotnetRunMessages\u0026#34;: true, \u0026#34;launchBrowser\u0026#34;: true, \u0026#34;launchUrl\u0026#34;: \u0026#34;weatherforecast\u0026#34;, \u0026#34;applicationUrl\u0026#34;: \u0026#34;http://localhost:5217\u0026#34;, \u0026#34;environmentVariables\u0026#34;: { \u0026#34;ASPNETCORE_ENVIRONMENT\u0026#34;: \u0026#34;Development\u0026#34; } }, \u0026#34;Dapr-PWSH\u0026#34;: { \u0026#34;commandName\u0026#34;: \u0026#34;Executable\u0026#34;, \u0026#34;executablePath\u0026#34;: \u0026#34;pwsh\u0026#34;, \u0026#34;commandLineArgs\u0026#34;: \u0026#34;-Command \\\u0026#34;dapr run -a myapp -p 5217 -- dotnet run --no-build\\\u0026#34;\u0026#34;, \u0026#34;workingDirectory\u0026#34;: \u0026#34;.\u0026#34;, \u0026#34;environmentVariables\u0026#34;: { \u0026#34;ASPNETCORE_ENVIRONMENT\u0026#34;: \u0026#34;Development\u0026#34; }, \u0026#34;nativeDebugging\u0026#34;: true } } } 配置调试插件\n打开 调试 -\u0026gt; 其他调试目标 -\u0026gt; Child Process Debugging Settings...\n启用Enable child process debugging 并设置表格内容如下\nEnabled Process name Action Debugger Type ✅ \u0026lt;All other processeses\u0026gt; Do not debug \u0026lt;Inherit from parent process\u0026gt; ✅ dapr.exe Attach debugger Native ✅ dotnet.exe Attach debugger Native ✅ \u0026lt;your_program\u0026gt;.exe Attach debugger Managed (.NET Core, .NET 5+) 开始调试\n打上断点，启动配置选择 Dapr-PWSH，按下 F5 即可使用 dapr 启动程序并进行调试 使用 curl 调用 API 看能否进入断点\ncurl http://localhost:5217/weatherforcast 通过 Requeset.Headers.UserAgent 可以看出此请求来源是 curl\n使用 dapr CLI 调用看能否进入断点\ndapr invoke -a myapp -m weatherforcast -v GET dapr 使用 go 语言编写，所以 Requeset.Headers.UserAgent 显示 Go-http-client/1.1\n此方法调试的优点是简单，需要调试时候按下 F5 即可调试，和普通程序调试没有区别，但是我本人在使用过程中发现此方法调试过程有点卡顿，具体原因不知\n附加进程调试 此方法则是使用 VS 自带的附加调试功能，无需安装多余的软件和扩展\n使用 dapr 启动程序\ndapr run -a myapp -p 5217 -- dotnet run 通过菜单或使用快捷键 Ctrl+Alt+P 打开附加调试\n选择程序对应的进程，可以使用搜索窗口进行过滤\n此时 Visual Studio 就会进入调试状态，通过 curl 或 dapr 调用 api 都能进入断点\n如果 VS 已经附加过进程，可以通过 Shift+Alt+P 快速附加到上一次选择的同名进程\n此方法和使用 VS 扩展相比，启动和调试分成了两个操作，在面对频繁的启动调试时可能比较繁琐，但是我本人用此方法调试不会感觉到卡顿\n附加调试不仅能调试 dapr 应用程序，还支持许多更复杂的调试操作\n","permalink":"https://blog.fissssssh.com/posts/debug-program-start-by-dapr-with-visual-studio/","summary":"dapr 应用程序是一个多进程程序，对于 Visual Studio 来说可能比较困难\n这里介绍两种方法来使用 Visual Studio 调试 dapr 应用程序\n使用 VS 扩展 附加进程调试 使用 VS 扩展 参考 https://github.com/dapr/dotnet-sdk/issues/401#issuecomment-747563695\n准备条件 安装 PowerShell 7 / Core 也可使用 dotnet tool install --global PowerShell 进行安装\n安装 VS 扩展 Microsoft Child Process Debugging Power Tool 2022 调试准备 编辑launchSettings.json\n{ \u0026#34;$schema\u0026#34;: \u0026#34;https://json.schemastore.org/launchsettings.json\u0026#34;, \u0026#34;profiles\u0026#34;: { \u0026#34;WebApplication1\u0026#34;: { \u0026#34;commandName\u0026#34;: \u0026#34;Project\u0026#34;, \u0026#34;dotnetRunMessages\u0026#34;: true, \u0026#34;launchBrowser\u0026#34;: true, \u0026#34;launchUrl\u0026#34;: \u0026#34;weatherforecast\u0026#34;, \u0026#34;applicationUrl\u0026#34;: \u0026#34;http://localhost:5217\u0026#34;, \u0026#34;environmentVariables\u0026#34;: { \u0026#34;ASPNETCORE_ENVIRONMENT\u0026#34;: \u0026#34;Development\u0026#34; } }, \u0026#34;Dapr-PWSH\u0026#34;: { \u0026#34;commandName\u0026#34;: \u0026#34;Executable\u0026#34;, \u0026#34;executablePath\u0026#34;: \u0026#34;pwsh\u0026#34;, \u0026#34;commandLineArgs\u0026#34;: \u0026#34;-Command \\\u0026#34;dapr run -a myapp -p 5217 -- dotnet run --no-build\\\u0026#34;\u0026#34;, \u0026#34;workingDirectory\u0026#34;: \u0026#34;.","title":"使用 Visual Studio 调试 dapr 应用程序"},{"content":"定义 subpub 组件 我们使用 Dapr 初始化时安装的 redis 作为 pubsub 的实现\n创建文件 ~/.dapr/components/pubsub.yaml （Windows 用户为 %USERPROFILE%\\.dapr\\components\\pubsub.yaml ），内容如下\nDapr 初始化后 ~/.dapr/components 文件夹会自动创建，里面有一个 statestore.yaml 的组件定义。如果没有该文件夹也不用担心，手动创建即可\napiVersion: dapr.io/v1alpha1 kind: Component metadata: name: pubsub spec: type: pubsub.redis version: v1 metadata: - name: redisHost value: localhost:6379 - name: redisPassword value: \u0026#34;\u0026#34; 创建项目 创建 ASP.NET Core WebAPI 项目\n$ dotnet new webapi --no-openapi --no-https 安装 Dapr SDK\ndotnet CLI\n$ dotnet add package Dapr.AspNetCore 程序包管理器控制台\nInstall-Package Dapr.AspNetCore 也可以在 Visual Studio 的 Nuget 包管理器中搜索安装\n添加 Dapr 支持到 ASP.NET Core 框架\nProgram.cs\n- builder.Services.AddControllers(); + builder.Services.AddControllers().AddDapr(); app.MapControllers(); + app.MapSubscribeHandler(); 添加消息处理程序\nPubSubController.cs\nusing Dapr; using Microsoft.AspNetCore.Mvc; namespace pubsub { public class PubSubController : ControllerBase { private readonly ILogger\u0026lt;PubSubController\u0026gt; _logger; public PubSubController(ILogger\u0026lt;PubSubController\u0026gt; logger) { _logger = logger; } [Topic(\u0026#34;pubsub\u0026#34;, \u0026#34;Test\u0026#34;)] [HttpPost(\u0026#34;test\u0026#34;)] public async Task\u0026lt;IActionResult\u0026gt; SubscribeTest() { var sr = new StreamReader(HttpContext.Request.Body); var body = await sr.ReadToEndAsync(); _logger.LogInformation(\u0026#34;received cloud event from Test: {message}\u0026#34;, body); return Ok(); } } } 使用 Dapr.AspNetCore 提供的 TopicAttribute 可以方便的订阅主题并关联到 API 接口，必须在管道中配置 app.MapSubscribeHandler()， 否则即使标注了 Topic 特性程序也不会订阅\n订阅主题的接口必须返回 HTTP 200 OK 状态码，否则 dapr 可能会视为消息推送不成功而进行重新推送，dapr 的消息推送规则为 AtLeastOnce，即至少一次\n测试订阅 使用 dapr 启动程序\n$ dapr run --app-id pubsubtest --app-port 5265 -- dotnet run --app-port 为应用程序的 http 端口，对于 ASP.NET Core 开发环境来说，一般在 Properties/launchSettings.json 中可以找到\n--app-id 为应用程序 id，同一组应用程序类似在发布订阅中类似消息队列中的消费者组，如果有多个程序的 app-id 相同，则只有一个程序实例能够收到消息\n使用 dapr 模拟发布\n$ dapr publish -i pubsubtest -p pubsub -t Test -d \u0026#39;{\u0026#34;data\u0026#34;:\u0026#34;this is a test message\u0026#34;}\u0026#39; Event published successfully 观察第 1 步中程序的输出\n== APP == info: pubsub.PubSubController[0] == APP == received cloud event from Test: {\u0026#34;data\u0026#34;:{\u0026#34;data\u0026#34;:\u0026#34;this is a test message\u0026#34;},\u0026#34;datacontenttype\u0026#34;:\u0026#34;application/json\u0026#34;,\u0026#34;id\u0026#34;:\u0026#34;5d485363-1658-4771-8c25-a84615359dde\u0026#34;,\u0026#34;pubsubname\u0026#34;:\u0026#34;pubsub\u0026#34;,\u0026#34;source\u0026#34;:\u0026#34;pubsubtest\u0026#34;,\u0026#34;specversion\u0026#34;:\u0026#34;1.0\u0026#34;,\u0026#34;time\u0026#34;:\u0026#34;2022-12-15T01:17:10+08:00\u0026#34;,\u0026#34;topic\u0026#34;:\u0026#34;Test\u0026#34;,\u0026#34;traceid\u0026#34;:\u0026#34;00-363f742582302ed141396ce408dbacc0-4b6f5751d4039d75-01\u0026#34;,\u0026#34;traceparent\u0026#34;:\u0026#34;00-363f742582302ed141396ce408dbacc0-4b6f5751d4039d75-01\u0026#34;,\u0026#34;tracestate\u0026#34;:\u0026#34;\u0026#34;,\u0026#34;type\u0026#34;:\u0026#34;com.dapr.event.sent\u0026#34;} 可以发现收到的数据是一个 JSON 格式数据， 其中 data 字段正是我们实际发送的内容\n{ \u0026#34;data\u0026#34;: { \u0026#34;data\u0026#34;: \u0026#34;this is a test message\u0026#34; }, \u0026#34;datacontenttype\u0026#34;: \u0026#34;application/json\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;5d485363-1658-4771-8c25-a84615359dde\u0026#34;, \u0026#34;pubsubname\u0026#34;: \u0026#34;pubsub\u0026#34;, \u0026#34;source\u0026#34;: \u0026#34;pubsubtest\u0026#34;, \u0026#34;specversion\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;time\u0026#34;: \u0026#34;2022-12-15T01:17:10+08:00\u0026#34;, \u0026#34;topic\u0026#34;: \u0026#34;Test\u0026#34;, \u0026#34;traceid\u0026#34;: \u0026#34;00-363f742582302ed141396ce408dbacc0-4b6f5751d4039d75-01\u0026#34;, \u0026#34;traceparent\u0026#34;: \u0026#34;00-363f742582302ed141396ce408dbacc0-4b6f5751d4039d75-01\u0026#34;, \u0026#34;tracestate\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;com.dapr.event.sent\u0026#34; } 这种数据格式在 dapr 中称为 CloudEvent，它记录了发送的内容，类型，主题和其他元信息，大部分情况下我们只需要 data 字段的内容，可以通过在 ASP.NET Core 管道中添加如下代码来对 CloudEvent 消息解包：\n+ app.UseCloudEvents(); app.MapControllers(); app.MapSubscribeHandler(); 修改代码后重新启动程序并发布测试消息，可以观察到以下输出\n== APP == info: pubsub.PubSubController[0] == APP == received cloud event from Test: {\u0026#34;data\u0026#34;:\u0026#34;this is a test message\u0026#34;} 实际情况中不需要自己从 HttpContext.Request.Body 中读取数据，可以通过 ASP.NET Core 的模型绑定来做\n测试发布 修改现有代码\nprivate readonly ILogger\u0026lt;PubSubController\u0026gt; _logger; + private readonly DaprClient _daprClient; - public PubSubController(ILogger\u0026lt;PubSubController\u0026gt; logger) + public PubSubController(ILogger\u0026lt;PubSubController\u0026gt; logger, DaprClient daprClient) { _logger = logger; + _daprClient = daprClient; } + [HttpPost(\u0026#34;pub\u0026#34;)] + public async Task\u0026lt;IActionResult\u0026gt; PublishTest() + { + await _daprClient.PublishEventAsync(\u0026#34;pubsub\u0026#34;, \u0026#34;Test\u0026#34;, new { data = \u0026#34;this is a test publish message\u0026#34; }); + return Ok(); + } 调用发布消息的 API 接口\n$ curl -XPOST http://localhost:5265/pub 观察程序输出\n== APP == info: pubsub.PubSubController[0] == APP == received cloud event from Test: {\u0026#34;data\u0026#34;:\u0026#34;this is a test publish message\u0026#34;} 调用发布 API 后，程序发布了消息到 pubsub 组件的 Test 组件，而我们的程序又订阅了这个主题，所以会打印出我们发布出去的消息\n","permalink":"https://blog.fissssssh.com/posts/dapr-publish-and-subscription-in-asp-net-core/","summary":"定义 subpub 组件 我们使用 Dapr 初始化时安装的 redis 作为 pubsub 的实现\n创建文件 ~/.dapr/components/pubsub.yaml （Windows 用户为 %USERPROFILE%\\.dapr\\components\\pubsub.yaml ），内容如下\nDapr 初始化后 ~/.dapr/components 文件夹会自动创建，里面有一个 statestore.yaml 的组件定义。如果没有该文件夹也不用担心，手动创建即可\napiVersion: dapr.io/v1alpha1 kind: Component metadata: name: pubsub spec: type: pubsub.redis version: v1 metadata: - name: redisHost value: localhost:6379 - name: redisPassword value: \u0026#34;\u0026#34; 创建项目 创建 ASP.NET Core WebAPI 项目\n$ dotnet new webapi --no-openapi --no-https 安装 Dapr SDK\ndotnet CLI\n$ dotnet add package Dapr.AspNetCore 程序包管理器控制台\nInstall-Package Dapr.AspNetCore 也可以在 Visual Studio 的 Nuget 包管理器中搜索安装","title":"ASP.NET Core 中使用 Dapr 发布订阅"},{"content":"前置条件 Dapr 可以脱离 Docker 运行，但不在本篇所讲范围内，本篇内容中的操作都是基于 Docker 安装完成并运行正常的情况下的操作\nDocker Docker 安装官方文档描述十分清晰。 Windows 用户推荐安装带界面的 Docker Desktop（更符合 Windows 人的操作习惯吧），Linux 用户安装 Docker Engine 即可。\n开始安装 Dapr 的安装分为两部分：\n安装 Dapr CLI 安装 Dapr Runtime 有两种方式可以进行安装，在线安装方式会去 github 下载对应的资源，网络不好的同学可以使用离线安装的方式\n在线安装 安装 Dapr CLI\nDapr 官网有各个操作系统详细的安装方法，这里讲一下通用的二进制安装方法：\n首先去 Dapr CLI 的发布页（目前最新是 1.9.1 版本）下载对应操作系统版本的压缩包，命名格式为dapr_\u0026lt;os_name\u0026gt;_\u0026lt;cpu_arch\u0026gt;.(tar.gz|zip)，如果操作系统或者 CPU 架构没选对，则 Dapr CLI 无法正常运行 通常来说 Windows 用户下载dapr_windows_amd64.zip，Linux 用户下载dapr_linux_amd64.tar.gz\n解压到任意文件夹，并将该文件夹路径加入PATH环境变量 Linux 用户可直接创建软连接到/usr/local/bin/dapr， Windows 用户推荐将文件解压至%USERPROFILE%\\bin\\文件夹，并将%USERPROFILE%\\bin\\添加到PATH环境变量中，后续有其他的可执行文件也可以放入该文件夹，不用再动环境变量\n打开控制台或者终端输入dapr，如果有相关的内容输出则安装成功 安装 Dapr Runtime\n安装 Dapr Runtime 也叫初始化 Dapr。\n本地 Dapr 环境的初始化很简单， 只需要dapr init即可\n离线安装 下载Dapr Install-Bundle，下载文件的选择同 Dapr CLI\n解压，并将 daprbundle/dapr（dapr.exe）移动到可执行文件夹下（参考在线安装中安装 Dapr CLI 的第 2 步）\n移动到 daprbundle 文件夹下执行 dapr init --from-dir .\n离线安装不会安装 zipkin 和 redis，可以使用下列指令进行安装\n$ docker run --name \u0026#34;dapr_zipkin\u0026#34; --restart always -d -p 9411:9411 openzipkin/zipkin $ docker run --name \u0026#34;dapr_redis\u0026#34; --restart always -d -p 6379:6379 redislabs/rejson 验证 打开控制台或终端输入dapr version，可以看到已经安装的版本\n$ dapr version CLI version: 1.9.1 Runtime version: 1.9.5 如果出现上述内容，恭喜您，您已经成功安装本地 dapr 环境！\n","permalink":"https://blog.fissssssh.com/posts/dapr/install/","summary":"前置条件 Dapr 可以脱离 Docker 运行，但不在本篇所讲范围内，本篇内容中的操作都是基于 Docker 安装完成并运行正常的情况下的操作\nDocker Docker 安装官方文档描述十分清晰。 Windows 用户推荐安装带界面的 Docker Desktop（更符合 Windows 人的操作习惯吧），Linux 用户安装 Docker Engine 即可。\n开始安装 Dapr 的安装分为两部分：\n安装 Dapr CLI 安装 Dapr Runtime 有两种方式可以进行安装，在线安装方式会去 github 下载对应的资源，网络不好的同学可以使用离线安装的方式\n在线安装 安装 Dapr CLI\nDapr 官网有各个操作系统详细的安装方法，这里讲一下通用的二进制安装方法：\n首先去 Dapr CLI 的发布页（目前最新是 1.9.1 版本）下载对应操作系统版本的压缩包，命名格式为dapr_\u0026lt;os_name\u0026gt;_\u0026lt;cpu_arch\u0026gt;.(tar.gz|zip)，如果操作系统或者 CPU 架构没选对，则 Dapr CLI 无法正常运行 通常来说 Windows 用户下载dapr_windows_amd64.zip，Linux 用户下载dapr_linux_amd64.tar.gz\n解压到任意文件夹，并将该文件夹路径加入PATH环境变量 Linux 用户可直接创建软连接到/usr/local/bin/dapr， Windows 用户推荐将文件解压至%USERPROFILE%\\bin\\文件夹，并将%USERPROFILE%\\bin\\添加到PATH环境变量中，后续有其他的可执行文件也可以放入该文件夹，不用再动环境变量\n打开控制台或者终端输入dapr，如果有相关的内容输出则安装成功 安装 Dapr Runtime\n安装 Dapr Runtime 也叫初始化 Dapr。\n本地 Dapr 环境的初始化很简单， 只需要dapr init即可","title":"安装 Dapr"},{"content":"引言 插件可以提高开发人员的生产力，好的插件可以让开发事半功倍，下面推荐一些自己用的 Visual Studio 插件（免费！）\n插件列表 以下所有插件在 Visual Studio 2022 运行正常，若版本不兼容请移步 Visual Studio Marketplace 查看是否有对应版本\n插件名称 描述 CodeMaid VS2022 代码清理插件，其中有一个码锹窗口深得我心 Output enhancer 输出窗口使用彩色优化，错误为红色，警告为黄色等 Code alignment 代码对齐，让你的某些代码按照某个字符垂直对齐 Double-Click Maximize 2022 双击最大化，不用先拖出来再放大，再次双击即可回归原位 Time Stamp Margin 2022 在调试窗口左边增加一列时间戳 Shrink Empty Lines 2022 压缩空行和没有字符的行（只有括号的行）的高度，从而在屏幕上显示更多代码 Solution Error Visualizer 2022 当某个文件有错误或者警告时，在解决方案资源管理器中显示（其父目录也会显示） Match Margin 2022 在滚动条上显示当前选中单词的位置 ","permalink":"https://blog.fissssssh.com/posts/recommended-visual-studio-plugins/","summary":"引言 插件可以提高开发人员的生产力，好的插件可以让开发事半功倍，下面推荐一些自己用的 Visual Studio 插件（免费！）\n插件列表 以下所有插件在 Visual Studio 2022 运行正常，若版本不兼容请移步 Visual Studio Marketplace 查看是否有对应版本\n插件名称 描述 CodeMaid VS2022 代码清理插件，其中有一个码锹窗口深得我心 Output enhancer 输出窗口使用彩色优化，错误为红色，警告为黄色等 Code alignment 代码对齐，让你的某些代码按照某个字符垂直对齐 Double-Click Maximize 2022 双击最大化，不用先拖出来再放大，再次双击即可回归原位 Time Stamp Margin 2022 在调试窗口左边增加一列时间戳 Shrink Empty Lines 2022 压缩空行和没有字符的行（只有括号的行）的高度，从而在屏幕上显示更多代码 Solution Error Visualizer 2022 当某个文件有错误或者警告时，在解决方案资源管理器中显示（其父目录也会显示） Match Margin 2022 在滚动条上显示当前选中单词的位置 ","title":"Visual Studio 2022 插件推荐"},{"content":"引言 有时候我们可能想在接口中开启一个后台任务，就像这样:\npublic class MyController : Controller { private readonly MyDependency _dep; public MyController(MyDependency d) { _dep = d; } public IActionResult MyAction() { Task.Run(() =\u0026gt; _dep.DoHeavyAsyncWork()); return Json(\u0026#34;Your job is started!\u0026#34;); } } 事实上，这段代码很符合语义，接口也能成功返回。 但是其中有一个潜在问题，随着return语句响应本次请求，ASP.NET Core会开始释放本次请求的依赖，比如控制器和控制器依赖的各种服务……，这就会导致Task.Run语句中的所依赖的定义在控制器中的服务会被释放掉，这可能会导致一些未知的问题。这些潜在问题可能会让你的应用程序随时爆炸！\n让后台任务拥有自己的生存期 为了保证后台服务的依赖不随控制器释放而释放，我们需要定义一个新的服务并将其注册为单例服务，因为单例服务永远不会被释放。\n创建一个名为CannonService的类\nCannonService.cs\npublic class CannonService { private readonly ILogger\u0026lt;CannonService\u0026gt; _logger; private readonly IServiceProvider _serviceProvider; public CannonService(ILogger\u0026lt;CannonService\u0026gt; logger, IServiceProvider serviceProvider) { _logger = logger; _serviceProvider = serviceProvider; } public void Fire(Delegate dg, Action\u0026lt;Exception\u0026gt;? exceptionHandler = null) { if (dg == null) { return; } Task.Run(async () =\u0026gt; { // 通过反射获取委托的参数列表 var parameterTypes = dg.Method.GetParameters().Select(x =\u0026gt; x.ParameterType).ToArray(); var parameters = ArrayPool\u0026lt;object\u0026gt;.Shared.Rent(parameterTypes.Length); try { // 创建一个scope using var scope = _serviceProvider.CreateScope(); // 获取委托的参数实例 for (var i = 0; i \u0026lt; parameterTypes.Length; i++) { var t = parameterTypes[i]; parameters[i] = scope.ServiceProvider.GetRequiredService(t); } // 判断委托是是异步还是同步 var returnType = dg.Method.ReturnType; if (returnType.IsAssignableTo(typeof(Task))) { await (Task)dg.DynamicInvoke(parameters.Take(parameterTypes.Length).ToArray())!; } else { dg.DynamicInvoke(parameters.Take(parameterTypes.Length).ToArray()); } } catch (Exception e) { _logger.LogError(e, \u0026#34;Fire boom!\u0026#34;); exceptionHandler?.Invoke(e); } finally { ArrayPool\u0026lt;object\u0026gt;.Shared.Return(parameters, true); } }); } } 将CannonService注册为单例服务 Program.cs\nbuilder.Services.AddSingleton\u0026lt;CannonService\u0026gt;(); 在需要的地方注入该服务并使用\n触发同步方法\n_cannonService.Fire((SomeService someService) =\u0026gt; { someService.From = nameof(CannonService); someService.DoSomething(); }); 触发异步方法\n_cannonService.Fire(async (SomeService someService) =\u0026gt; { someService.From = nameof(CannonService); await Task.Delay(timeout); someService.DoSomething(); }); 示例 SomeService.cs\nnamespace RunTaskInBackground.Demo { public class SomeService : IDisposable { private readonly ILogger\u0026lt;SomeService\u0026gt; _logger; private bool disposedValue; public string From { get; set; } = null!; public SomeService(ILogger\u0026lt;SomeService\u0026gt; logger) { _logger = logger; } public void DoSomething() { CheckIfClosedThrowDisposed(); _logger.LogInformation(\u0026#34;我从{From}来的，我在干活！\u0026#34;, From); } void CheckIfClosedThrowDisposed() { if (disposedValue) throw new ObjectDisposedException(null, \u0026#34;我歇逼了，别叫我！\u0026#34;); } protected virtual void Dispose(bool disposing) { if (!disposedValue) { if (disposing) { if (!string.IsNullOrEmpty(From)) { _logger.LogInformation(\u0026#34;我从{From}来的，我歇逼了！\u0026#34;, From); } else { _logger.LogInformation(\u0026#34;我没有🐎，我歇逼了！\u0026#34;); } } disposedValue = true; } } public void Dispose() { Dispose(disposing: true); GC.SuppressFinalize(this); } } } Program.cs\nusing RunTaskInBackground.Demo; var builder = WebApplication.CreateBuilder(args); // Add services to the container. builder.Services.AddControllers(); // Learn more about configuring Swagger/OpenAPI at https://aka.ms/aspnetcore/swashbuckle builder.Services.AddEndpointsApiExplorer(); builder.Services.AddSwaggerGen(); builder.Services.AddSingleton\u0026lt;CannonService\u0026gt;(); builder.Services.AddScoped\u0026lt;SomeService\u0026gt;(); var app = builder.Build(); // Configure the HTTP request pipeline. if (app.Environment.IsDevelopment()) { app.UseSwagger(); app.UseSwaggerUI(); } app.UseAuthorization(); app.MapControllers(); app.Run(); HomeController.cs\nusing Microsoft.AspNetCore.Mvc; namespace RunTaskInBackground.Demo.Controllers { [ApiController] [Route(\u0026#34;[controller]\u0026#34;)] public class HomeController : ControllerBase { private readonly ILogger\u0026lt;HomeController\u0026gt; _logger; private readonly CannonService _cannonService; private readonly SomeService _someService; public HomeController(ILogger\u0026lt;HomeController\u0026gt; logger, CannonService cannonService, SomeService someService) { _logger = logger; _cannonService = cannonService; _someService = someService; _someService.From = nameof(HomeController); } [HttpGet] [Route(\u0026#34;Async\u0026#34;)] public IActionResult RunAsync(int timeout = 1000) { _cannonService.Fire(async (SomeService someService) =\u0026gt; { someService.From = nameof(CannonService); await Task.Delay(timeout); someService.DoSomething(); }); return Ok(); } [HttpGet] [Route(\u0026#34;AsyncBoom\u0026#34;)] public IActionResult RunAsyncBoom(int timeout = 1000) { _cannonService.Fire(async () =\u0026gt; { await Task.Delay(timeout); _someService.DoSomething(); }); return Ok(); } [HttpGet] [Route(\u0026#34;Sync\u0026#34;)] public IActionResult RunSync() { _cannonService.Fire((SomeService someService) =\u0026gt; { someService.From = nameof(CannonService); someService.DoSomething(); }); return Ok(); } } } 在 swagger 中分别调用 Sync，Async，AsyncBoom 接口，结果如下图所示：\n附完整 Demo 地址\n参考 Anduin Xue.Fire and forget in ASP.NET Core with dependency alive[EB/OL].(2020-10-14)[2022-06-27].https://anduin.aiursoft.com/post/2020/10/14/fire-and-forget-in-aspnet-core-with-dependency-alive 本作品采用署名-相同方式共享 4.0 国际协议许可\n","permalink":"https://blog.fissssssh.com/posts/how-to-start-backgroud-task-in-asp_net_core_webapi/","summary":"引言 有时候我们可能想在接口中开启一个后台任务，就像这样:\npublic class MyController : Controller { private readonly MyDependency _dep; public MyController(MyDependency d) { _dep = d; } public IActionResult MyAction() { Task.Run(() =\u0026gt; _dep.DoHeavyAsyncWork()); return Json(\u0026#34;Your job is started!\u0026#34;); } } 事实上，这段代码很符合语义，接口也能成功返回。 但是其中有一个潜在问题，随着return语句响应本次请求，ASP.NET Core会开始释放本次请求的依赖，比如控制器和控制器依赖的各种服务……，这就会导致Task.Run语句中的所依赖的定义在控制器中的服务会被释放掉，这可能会导致一些未知的问题。这些潜在问题可能会让你的应用程序随时爆炸！\n让后台任务拥有自己的生存期 为了保证后台服务的依赖不随控制器释放而释放，我们需要定义一个新的服务并将其注册为单例服务，因为单例服务永远不会被释放。\n创建一个名为CannonService的类\nCannonService.cs\npublic class CannonService { private readonly ILogger\u0026lt;CannonService\u0026gt; _logger; private readonly IServiceProvider _serviceProvider; public CannonService(ILogger\u0026lt;CannonService\u0026gt; logger, IServiceProvider serviceProvider) { _logger = logger; _serviceProvider = serviceProvider; } public void Fire(Delegate dg, Action\u0026lt;Exception\u0026gt;?","title":"如何在 ASP.NET Core WEB API 中启动后台任务"},{"content":"Create a docker-compose.yml version: \u0026#34;3.9\u0026#34; services: redis-1: image: redis:7.0.2-alpine container_name: redis-1 ports: - \u0026#34;6371:6371\u0026#34; - \u0026#34;16371:16371\u0026#34; volumes: - ./node-1/data:/data - ./node-1/conf/redis.conf:/etc/redis/redis.conf networks: redis: ipv4_address: 172.28.0.11 command: - \u0026#34;redis-server\u0026#34; - \u0026#34;/etc/redis/redis.conf\u0026#34; redis-2: image: redis:7.0.2-alpine container_name: redis-2 ports: - \u0026#34;6372:6372\u0026#34; - \u0026#34;16372:16372\u0026#34; volumes: - ./node-2/data:/data - ./node-2/conf/redis.conf:/etc/redis/redis.conf networks: redis: ipv4_address: 172.28.0.12 command: - \u0026#34;redis-server\u0026#34; - \u0026#34;/etc/redis/redis.conf\u0026#34; redis-3: image: redis:7.0.2-alpine container_name: redis-3 ports: - \u0026#34;6373:6373\u0026#34; - \u0026#34;16373:16373\u0026#34; volumes: - ./node-3/data:/data - ./node-3/conf/redis.conf:/etc/redis/redis.conf networks: redis: ipv4_address: 172.28.0.13 command: - \u0026#34;redis-server\u0026#34; - \u0026#34;/etc/redis/redis.conf\u0026#34; redis-4: image: redis:7.0.2-alpine container_name: redis-4 ports: - \u0026#34;6374:6374\u0026#34; - \u0026#34;16374:16374\u0026#34; volumes: - ./node-4/data:/data - ./node-4/conf/redis.conf:/etc/redis/redis.conf networks: redis: ipv4_address: 172.28.0.14 command: - \u0026#34;redis-server\u0026#34; - \u0026#34;/etc/redis/redis.conf\u0026#34; redis-5: image: redis:7.0.2-alpine container_name: redis-5 ports: - \u0026#34;6375:6375\u0026#34; - \u0026#34;16375:16375\u0026#34; volumes: - ./node-5/data:/data - ./node-5/conf/redis.conf:/etc/redis/redis.conf networks: redis: ipv4_address: 172.28.0.15 command: - \u0026#34;redis-server\u0026#34; - \u0026#34;/etc/redis/redis.conf\u0026#34; redis-6: image: redis:7.0.2-alpine container_name: redis-6 ports: - \u0026#34;6376:6376\u0026#34; - \u0026#34;16376:16376\u0026#34; volumes: - ./node-6/data:/data - ./node-6/conf/redis.conf:/etc/redis/redis.conf networks: redis: ipv4_address: 172.28.0.16 command: - \u0026#34;redis-server\u0026#34; - \u0026#34;/etc/redis/redis.conf\u0026#34; redis-cluster: image: redis:7.0.2-alpine networks: redis: ipv4_address: 172.28.0.17 depends_on: - redis-1 - redis-2 - redis-3 - redis-4 - redis-5 - redis-6 command: - \u0026#34;redis-cli\u0026#34; - \u0026#34;--cluster\u0026#34; - \u0026#34;create\u0026#34; - \u0026#34;172.28.0.11:6371\u0026#34; - \u0026#34;172.28.0.12:6372\u0026#34; - \u0026#34;172.28.0.13:6373\u0026#34; - \u0026#34;172.28.0.14:6374\u0026#34; - \u0026#34;172.28.0.15:6375\u0026#34; - \u0026#34;172.28.0.16:6376\u0026#34; - \u0026#34;--cluster-replicas\u0026#34; - \u0026#34;1\u0026#34; - \u0026#34;--cluster-yes\u0026#34; networks: redis: ipam: config: - subnet: \u0026#34;172.28.0.0/16\u0026#34; There has a container named redis-cluster, the container is for create cluster for redis nodes, if you dont like this, you can enter any redis node container and execute the following command\nredis-cli --cluster create \\ 172.28.0.11:6371 \\ 172.28.0.12:6372 \\ 172.28.0.13:6373 \\ 172.28.0.14:6374 \\ 172.28.0.15:6375 \\ 172.28.0.16:6376 \\ --cluster-replicas 1 --cluster-yes Create config files for every redis node Run the following commands to create config files\nfor port in $(seq 1 6); \\ do \\ mkdir -p ./node-${port}/conf touch ./node-${port}/conf/redis.conf cat \u0026lt;\u0026lt; EOF \u0026gt; ./node-${port}/conf/redis.conf port 637${port} bind 0.0.0.0 cluster-enabled yes cluster-config-file nodes.conf cluster-node-timeout 5000 cluster-announce-ip \u0026lt;your_ip\u0026gt; cluster-announce-port 637${port} cluster-announce-bus-port 1637${port} appendonly yes EOF done Don\u0026rsquo;t forget to provide your actual IP address instead of \u0026lt;your_ip\u0026gt; (f.i. 10.12.123.124). Otherwise nodes wouldn\u0026rsquo;t join into cluster.\nYou can change port range if nesscessary\nfor port in $(seq 1 6); \\ ... -port 637${port} +port \u0026lt;port_prefix\u0026gt;${port} ... -cluster-announce-port 637${port} -cluster-announce-bus-port 1637${port} +cluster-announce-port \u0026lt;port_prefix\u0026gt;${port} +cluster-announce-bus-port \u0026lt;cluster_announce_bus_port_prefix\u0026gt;${port} ... done It also need to change docker-compose.yml\nversion: \u0026#34;3.9\u0026#34; services: redis-1: ... ports: - - \u0026#34;6371:6371\u0026#34; - - \u0026#34;16371:16371\u0026#34; + - \u0026#34;\u0026lt;port_prefix\u0026gt;1:\u0026lt;port_prefix\u0026gt;1\u0026#34; + - \u0026#34;\u0026lt;cluster_announce_bus_port_prefix\u0026gt;1:\u0026lt;cluster_announce_bus_port_prefix\u0026gt;1\u0026#34; ... redis-2: ... ports: - - \u0026#34;6372:6372\u0026#34; - - \u0026#34;16372:16372\u0026#34; + - \u0026#34;\u0026lt;port_prefix\u0026gt;:\u0026lt;port_prefix\u0026gt;\u0026#34; + - \u0026#34;\u0026lt;cluster_announce_bus_port_prefix\u0026gt;2:\u0026lt;cluster_announce_bus_port_prefix\u0026gt;2\u0026#34; ... redis-3: ... ports: - - \u0026#34;6373:6373\u0026#34; - - \u0026#34;16373:16373\u0026#34; + - \u0026#34;\u0026lt;port_prefix\u0026gt;3:\u0026lt;port_prefix\u0026gt;3\u0026#34; + - \u0026#34;\u0026lt;cluster_announce_bus_port_prefix\u0026gt;3:\u0026lt;cluster_announce_bus_port_prefix\u0026gt;3\u0026#34; ... redis-4: ... ports: - - \u0026#34;6374:6374\u0026#34; - - \u0026#34;16374:16374\u0026#34; + - \u0026#34;\u0026lt;port_prefix\u0026gt;4:\u0026lt;port_prefix\u0026gt;4\u0026#34; + - \u0026#34;\u0026lt;cluster_announce_bus_port_prefix\u0026gt;4:\u0026lt;cluster_announce_bus_port_prefix\u0026gt;4\u0026#34; ... redis-5: ... ports: - - \u0026#34;6375:6375\u0026#34; - - \u0026#34;16375:16375\u0026#34; + - \u0026#34;\u0026lt;port_prefix\u0026gt;5:\u0026lt;port_prefix\u0026gt;5\u0026#34; + - \u0026#34;\u0026lt;cluster_announce_bus_port_prefix\u0026gt;5:\u0026lt;cluster_announce_bus_port_prefix\u0026gt;5\u0026#34; ... redis-6: ... ports: - - \u0026#34;6376:6376\u0026#34; - - \u0026#34;16376:16376\u0026#34; + - \u0026#34;\u0026lt;port_prefix\u0026gt;6:\u0026lt;port_prefix\u0026gt;6\u0026#34; + - \u0026#34;\u0026lt;cluster_announce_bus_port_prefix\u0026gt;6:\u0026lt;cluster_announce_bus_port_prefix\u0026gt;6\u0026#34; ... redis-cluster: ... command: ... - - \u0026#34;172.28.0.11:6371\u0026#34; - - \u0026#34;172.28.0.12:6372\u0026#34; - - \u0026#34;172.28.0.13:6373\u0026#34; - - \u0026#34;172.28.0.14:6374\u0026#34; - - \u0026#34;172.28.0.15:6375\u0026#34; - - \u0026#34;172.28.0.16:6376\u0026#34; + - \u0026#34;172.28.0.11:\u0026lt;port_prefix\u0026gt;1\u0026#34; + - \u0026#34;172.28.0.12:\u0026lt;port_prefix\u0026gt;2\u0026#34; + - \u0026#34;172.28.0.13:\u0026lt;port_prefix\u0026gt;3\u0026#34; + - \u0026#34;172.28.0.14:\u0026lt;port_prefix\u0026gt;4\u0026#34; + - \u0026#34;172.28.0.15:\u0026lt;port_prefix\u0026gt;5\u0026#34; + - \u0026#34;172.28.0.16:\u0026lt;port_prefix\u0026gt;6\u0026#34; ... Start redis cluster Just run command docker-compose up -d.\n","permalink":"https://blog.fissssssh.com/posts/create-redis-cluster-by-docker-compose/","summary":"Create a docker-compose.yml version: \u0026#34;3.9\u0026#34; services: redis-1: image: redis:7.0.2-alpine container_name: redis-1 ports: - \u0026#34;6371:6371\u0026#34; - \u0026#34;16371:16371\u0026#34; volumes: - ./node-1/data:/data - ./node-1/conf/redis.conf:/etc/redis/redis.conf networks: redis: ipv4_address: 172.28.0.11 command: - \u0026#34;redis-server\u0026#34; - \u0026#34;/etc/redis/redis.conf\u0026#34; redis-2: image: redis:7.0.2-alpine container_name: redis-2 ports: - \u0026#34;6372:6372\u0026#34; - \u0026#34;16372:16372\u0026#34; volumes: - ./node-2/data:/data - ./node-2/conf/redis.conf:/etc/redis/redis.conf networks: redis: ipv4_address: 172.28.0.12 command: - \u0026#34;redis-server\u0026#34; - \u0026#34;/etc/redis/redis.conf\u0026#34; redis-3: image: redis:7.0.2-alpine container_name: redis-3 ports: - \u0026#34;6373:6373\u0026#34; - \u0026#34;16373:16373\u0026#34; volumes: - ./node-3/data:/data - ./node-3/conf/redis.conf:/etc/redis/redis.conf networks: redis: ipv4_address: 172.","title":"Create Redis Cluster by Docker Compose"},{"content":"问题描述 近日我使用 hugo 构建了我的博客，并通过 Github Action 将其发布在 Github Pages 上，刚开始还是很美好的，但是过一短时间以后打开页面发现样式全无，使用浏览器的开发者工具查看资源获取没有问题，但是在控制台却出现了这样一句话：\nFailed to find a valid digest in the \u0026#39;integrity\u0026#39; attribute for resource \u0026#39;***\u0026#39; with computed SHA-256 integrity \u0026#39;***\u0026#39;. The resource has been blocked. 寻找原因 我在 MDN 上寻找到了关于 integrity 的定义，大概描述就是这是一个签名，浏览器获取到相应资源后会用相同的方法计算一个签名，只有签名相同时才会加载对应的资源，如果两个签名不一致则是文件完整性被破坏（文件发生了改变）。\n问题来了，整个发布过程是由 Github Action 全自动操作的，没有人为干预，文件为何会无缘无故改变呢？\n答案是 Cloudflare。 Cloudflare 中默认会开启静态资源的缓存来提高网站的加载速度，可是为什么缓存会改变文件呢？缓存并不会改变文件，在 Cloudflare 的 Speed \u0026gt; Optimization 中有一个叫 Auto Minify 的选项，描述如下：\nReduce the file size of source code on your website.\nNote: Purge cache to have your change take effect immediately.\n这句话翻译过来就是 减小网站源代码文件体积的大小 ，同时该选项有 3 个子选项：\nJavaScript CSS HTML 默认都是选中状态，即默认会压缩我们的 js，css 和 html 文件。\n解决问题 因为我们网站是 css 文件的签名验证出现了问题，因此我们取消 Auto Minify 下面的 Css 选项，然后在Caching \u0026gt; Configuration 中点击 Purge Cache 清除缓存，回到网站使用 ctrl + F5 强制刷新网页，问题解决。\n其他解决方案 我使用的是 PaperMod 主题，可以在配置中关闭校验：\nconfig.yaml\nparams: assets: disableFingerprinting: true 关闭校验后在生成页面的时候 css 文件引用就不会有 integrity 属性\n结束语 遇到困难仔细分析，终会迎刃而解。\n","permalink":"https://blog.fissssssh.com/posts/css-integrity-error-when-load-hugo-publish/","summary":"问题描述 近日我使用 hugo 构建了我的博客，并通过 Github Action 将其发布在 Github Pages 上，刚开始还是很美好的，但是过一短时间以后打开页面发现样式全无，使用浏览器的开发者工具查看资源获取没有问题，但是在控制台却出现了这样一句话：\nFailed to find a valid digest in the \u0026#39;integrity\u0026#39; attribute for resource \u0026#39;***\u0026#39; with computed SHA-256 integrity \u0026#39;***\u0026#39;. The resource has been blocked. 寻找原因 我在 MDN 上寻找到了关于 integrity 的定义，大概描述就是这是一个签名，浏览器获取到相应资源后会用相同的方法计算一个签名，只有签名相同时才会加载对应的资源，如果两个签名不一致则是文件完整性被破坏（文件发生了改变）。\n问题来了，整个发布过程是由 Github Action 全自动操作的，没有人为干预，文件为何会无缘无故改变呢？\n答案是 Cloudflare。 Cloudflare 中默认会开启静态资源的缓存来提高网站的加载速度，可是为什么缓存会改变文件呢？缓存并不会改变文件，在 Cloudflare 的 Speed \u0026gt; Optimization 中有一个叫 Auto Minify 的选项，描述如下：\nReduce the file size of source code on your website.\nNote: Purge cache to have your change take effect immediately.","title":"Css Integrity Error When Load Hugo Publish"},{"content":"Introduction VSCode(Visual Studio Code) is a code editor redefined and optimized for building and debugging modern web and cloud applications.\nCode Snippets is a piece of code, the editor will auto complete a specific code when developer type some specific prefixes.\nCustom Code Snippets VSCode has many built-in code snippets, so I won\u0026rsquo;t go into details here, but mainly talk about how to customize code snippets.\nThere are two types of code snippets, one is global and the other is workspace, the difference between them is that the scope and storage location.\nglobal code snippets\nAny file opened by vscode can take effect and is stored in the installation directory.\nworkspace code snippets\nOnly the files in the workspace can take effect and are stored in the .vscode directory of the workspace.\nHow to create custom code snippets? Click File \u0026gt; Preferences \u0026gt; Configure User Snippets.\nClick New Global Snippets file... or New Snippets file for '\u0026lt;your_workspce_name\u0026gt;'....\nCode snippets store as json, Each snippet is defined under a snippet name and has a scope, prefix, body and description:\nscope: A comma separated ids of the languages where the snippet is applicable. If scope is left empty or omitted, the snippet gets applied to all languages prefix: It\u0026rsquo;s used to trigger the snippet and the body will be expanded and inserted. Possible variables are: $1, $2 for tab stops, $0 for the final cursor position, and ${1:label}, ${2:another} for placeholders body: The code description: The description of this code snippet Example:\n{ \u0026#34;scope\u0026#34;: \u0026#34;c\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;smain\u0026#34;, \u0026#34;body\u0026#34;: [ \u0026#34;#include \u0026lt;stdio.h\u0026gt;\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;int main(void)\u0026#34;, \u0026#34;{\u0026#34;, \u0026#34;\\t${1:/* Code here */}\u0026#34;, \u0026#34;\\treturn 0;\u0026#34;, \u0026#34;}\u0026#34; ] } A code snippets can store many code snippets, the root is a Object, a property corresponds to a code snippet\nExample:\n{ \u0026#34;Simple main\u0026#34;: { \u0026#34;scope\u0026#34;: \u0026#34;c\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;smain\u0026#34;, \u0026#34;body\u0026#34;: [ \u0026#34;#include \u0026lt;stdio.h\u0026gt;\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;int main(void)\u0026#34;, \u0026#34;{\u0026#34;, \u0026#34;\\t${1:/* Code here */}\u0026#34;, \u0026#34;\\treturn 0;\u0026#34;, \u0026#34;}\u0026#34; ] }, \u0026#34;Simple main c++\u0026#34;: { \u0026#34;scope\u0026#34;: \u0026#34;cpp\u0026#34;, \u0026#34;prefix\u0026#34;: \u0026#34;smainpp\u0026#34;, \u0026#34;body\u0026#34;: [ \u0026#34;#include \u0026lt;iostream\u0026gt;\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;using namespace std;\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;int main()\u0026#34;, \u0026#34;{\u0026#34;, \u0026#34;\\t${1:/* Code here */}\u0026#34;, \u0026#34;\\treturn 0;\u0026#34;, \u0026#34;}\u0026#34; ] } } Use code snippets\nRemarks Reasonable use of code snippets can greatly improve our coding efficiency\n","permalink":"https://blog.fissssssh.com/posts/code-snippets.in-vscode/","summary":"Introduction VSCode(Visual Studio Code) is a code editor redefined and optimized for building and debugging modern web and cloud applications.\nCode Snippets is a piece of code, the editor will auto complete a specific code when developer type some specific prefixes.\nCustom Code Snippets VSCode has many built-in code snippets, so I won\u0026rsquo;t go into details here, but mainly talk about how to customize code snippets.\nThere are two types of code snippets, one is global and the other is workspace, the difference between them is that the scope and storage location.","title":"Code Snippets in VSCode"},{"content":"以下是我本人平时喜欢用的软件，排名不分先后\n名称 描述 类型 7-Zip 压缩软件 desktop Fiddler HTTP 抓包工具 desktop Git 代码管理软件 cli,desktop Google Chrome 顶级浏览器 desktop MPC-HC 视频播放器 desktop MSI Afterburner 性能监视超频软件 desktop MacType Windows 字体渲染 desktop OBS Studio 推流工具 desktop PowerToys Windows 小工具集合 desktop Sourcetree 代码管理软件 desktop ToDesk 远程桌面工具 desktop Virtual Audio Cable 音频管理软件 desktop Visual Studio Code 吃饭的家伙 desktop Visual Studio 吃饭的家伙 desktop Voicemetter Banana 音频管理软件 desktop WinHex 文件查看工具 desktop WinSCP 文件复制工具 desktop WinaeroTweaker Windows 工具 desktop WireShark 网络抓包工具 desktop Wiztree 磁盘分析工具 desktop dnSpy .NET 反编译调试软件 desktop docker 容器环境 cli, desktop ffmpeg 流媒体处理工具 cli hugo 静态页面构建工具 cli 迅雷 下载工具 desktop ","permalink":"https://blog.fissssssh.com/posts/recommended-software/","summary":"以下是我本人平时喜欢用的软件，排名不分先后\n名称 描述 类型 7-Zip 压缩软件 desktop Fiddler HTTP 抓包工具 desktop Git 代码管理软件 cli,desktop Google Chrome 顶级浏览器 desktop MPC-HC 视频播放器 desktop MSI Afterburner 性能监视超频软件 desktop MacType Windows 字体渲染 desktop OBS Studio 推流工具 desktop PowerToys Windows 小工具集合 desktop Sourcetree 代码管理软件 desktop ToDesk 远程桌面工具 desktop Virtual Audio Cable 音频管理软件 desktop Visual Studio Code 吃饭的家伙 desktop Visual Studio 吃饭的家伙 desktop Voicemetter Banana 音频管理软件 desktop WinHex 文件查看工具 desktop WinSCP 文件复制工具 desktop WinaeroTweaker Windows 工具 desktop WireShark 网络抓包工具 desktop Wiztree 磁盘分析工具 desktop dnSpy .","title":"软件推荐"}]